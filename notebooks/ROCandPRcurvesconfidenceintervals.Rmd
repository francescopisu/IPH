---
title: "ROC and PR curves confidence intervals"
output: html_notebook
---

```{r}
install.packages('pROC')
install.packages("PRROC")
install.packages("ROCR")
install.packages("stringr")
install.packages("boot")
install.packages("MLmetrics")
install.packages("ggplot2")
install.packages("ggpubr")
install.packages("yardstick")
install.packages("ggsci")
install.packages("cvms")
install.packages("gridBase")
install.packages("gridExtra")
install.packages("dplyr")
install.packages("tidyr")
install.packages("reticulate")
```

```{r}
library("pROC")
library("PRROC")
library("ROCR")
library("stringr")
library("boot")
library("MLmetrics")
library("ggplot2")
library("ggpubr")
library("yardstick")
library("ggsci")
library("cvms")
library("gridBase")
library("gridExtra")
library("grid")
library("dplyr")
library("tidyr")
library("reticulate")
```

```{r}
use_condaenv("IPH", required = TRUE)
```

```{python}
def boot_prauc(preds_df_path):
  from sklearn.metrics import average_precision_score
  import pandas as pd
  import numpy as np
  import scikits.bootstrap as boot
  
  seed = 1303
  n_boots = 2000
  
  df = pd.read_csv(preds_df_path)
  
  conf_int, dist = boot.ci(data=(df["target"], df["proba"]),
                               statfunction=average_precision_score,
                               multi="paired",
                               n_samples=n_boots,
                               method="bca",
                               seed=seed,
                               return_dist=True)
  
  med = np.median(dist).item()
  
  #print(f"Metric name: {metric_name}, Median value:{med} ")
  test_prauc = "{:.3f} [{:.3f} - {:.3f}]".format(med, conf_int[0], conf_int[1])
  
  return test_prauc
```

# Load predicted probabilities on both training and test sets
```{r}
#external = read.csv('../output/predictions/external_test_preds_LGBM_tuned_merged_10outer_100.csv')
external = read.csv('../output/predictions/external_test_preds_LGBM_final.csv')
external.demo = read.csv('../output/predictions/external_test_preds_LGBM_only_demographics.csv')
```


```{r}
lightblue <- "#b0c4de"
salmon <- "#ff8c69"
```


# ROC curve with ggroc
```{r}
plot_roc_ci_ggroc <- function(df, target, proba) {
  # bootstrap AUCs for 95% CI
  set.seed(1303)
  
  fci<-function(data,indices,x,y){
     d<-as.data.frame(data[indices,])
     r<-MLmetrics::AUC(d[,2], d[,1])
     r
  }
  
  bootout <- boot(data=df,
                x=df[, target],
                y=df[, proba],
                R=2000,
                statistic=fci
                )
  
  roc.ci <- boot.ci(bootout, type="perc")
  roc.legend.text <- stringr::str_interp("Light GBM ($[.2f]{roc.ci$t0} [$[.2f]{roc.ci$percent[4]} - $[.2f]{roc.ci$percent[5]}])")
  
  # build ROC curve
  roc.obj <- pROC::roc(df[, target], df[, proba], plot=FALSE,
            legacy.axes=TRUE, percent=FALSE)
  ciobj <- ci.se(roc.obj, specificities = seq(0, 1, l = 25))
  dat.ci <- data.frame(x = as.numeric(rownames(ciobj)),
                       lower = ciobj[, 1],
                       upper = ciobj[, 3])
  
  
  ggroc(roc.obj, colour=salmon, linetype=1, size=1) + 
    theme(aspect.ratio = 1) +
    geom_abline(
        slope = 1,
        intercept = 1,
        linetype = "dashed",
        color = "grey"
    ) +
    geom_ribbon(
        data = dat.ci,
        aes(x = x, ymin = lower, ymax = upper),
        fill = lightblue,
        alpha = 0.2
    ) +
    coord_cartesian(ylim = c(0.0, 1.0)) +
    labs(title="ROC curve", x="1 - Specificity", y="Sensitivity", color="Legend title") +
    theme(
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(),
          panel.background = element_blank(), 
          panel.border = element_rect(colour="black", fill=NA, size=0.5),
          axis.title = element_text(size=14),
          axis.title.x = element_text(margin=margin(t=10)),
          axis.title.y = element_text(margin=margin(r=10)),
          plot.title = element_text(hjust=0.5, size=16, margin=margin(b=10)),
          legend.position = c(0.65, 0.12),
          legend.text = element_text(size=13)
          ) +
    scale_color_manual(name="Legend", values=c("Ciao"), labels=c("Pippo")) +
    scale_linetype_manual("pippo")
}

roc.curve <- plot_roc_ci(external, "target", "proba")
roc.curve
```


```{r}
target <- "target"
proba <- "proba"

roc.obj <- pROC::roc(external[, target], external[, proba], plot=FALSE,
            legacy.axes=TRUE, percent=FALSE)
coords(roc.obj, "best", best.method="youden", ret = c("threshold", "sensitivity", "specificity"), transpose = FALSE)
```




# ROC curve
```{r}
plot_roc_ci <- function(df, target, proba) {
  # bootstrap AUCs for 95% CI
  set.seed(1303)
  
  fci<-function(data,indices,x,y){
     d<-as.data.frame(data[indices,])
     r<-MLmetrics::AUC(d[,2], d[,1])
     r
  }
  
  bootout <- boot(data=df,
                x=df[, target],
                y=df[, proba],
                R=2000,
                statistic=fci
                )
  
  roc.ci <- boot.ci(bootout, type="perc")
  roc.legend.text <- stringr::str_interp("Light GBM ($[.2f]{roc.ci$t0} [$[.2f]{roc.ci$percent[4]} - $[.2f]{roc.ci$percent[5]}])")
  
  # build ROC curve
  roc.obj <- pROC::roc(df[, target], df[, proba], plot=FALSE,
            legacy.axes=TRUE, percent=FALSE)
  
  # extract specificities and sensitivities from the roc object
  spec <- rev(roc.obj$specificities)
  sens <- rev(roc.obj$sensitivities)
  dat <- as.data.frame(cbind(1-spec, sens))
  colnames(dat) <- c("specificity", "sensitivity")
  
  # bootstrap confidence intervals around sensitivities 
  ciobj <- ci.se(roc.obj, specificities = seq(0, 1, l = nrow(dat)))
  dat.ci <- data.frame(x = as.numeric(rownames(ciobj)),
                       lower = ciobj[, 1],
                       upper = ciobj[, 3])
  
  
  ggplot(dat, aes(x = specificity, y = sensitivity, linetype=roc.legend.text)) +
  geom_segment(aes(x = 0, y = 0, xend = 1,yend = 1), alpha = 0.5, color="gray",
               linetype="dashed") +
  geom_path(aes(specificity, sensitivity), colour=salmon, size=1) +
  geom_ribbon(
      aes(x = 1-dat.ci$x, ymin = dat.ci$lower, ymax = dat.ci$upper),
      fill = lightblue,
      alpha = 0.2
  ) +
  theme(aspect.ratio = 1) +
  labs(title="ROC curve", x="1 - Specificity", y="Sensitivity") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        panel.border = element_rect(colour="black", fill=NA, size=0.5),
        axis.title = element_text(size=16),
        axis.title.x = element_text(margin=margin(t=10)),
        axis.title.y = element_text(margin=margin(r=10)),
        plot.title = element_text(hjust=0.5, size=16, margin=margin(b=20)),
        legend.position = c(0.58, 0.12),
        legend.text = element_text(size=13)) +
  scale_linetype_manual(name='',
                 #breaks=c('Linear', 'Quadratic', 'Cubic'),
                 #values=c('Cubic'='pink', 'Quadratic'='blue', 'Linear'='purple')
                 values=c("solid")
                 )
}

roc.curve <- plot_roc_ci(external, "target", "proba")
roc.curve
```


# Precision-recall curve
```{r}
plot_pr_ci <- function(df, target, proba) {
  # bootstrap AUCs for 95% CI
  prauc.ci <- py$boot_prauc('../output/predictions/external_test_preds_LGBM_tuned_merged_10outer_100.csv')
  # $[.2f]{prauc.ci$t0} [$[.2f]{prauc.ci$percent[4]} - $[.2f]{prauc.ci$percent[5]}]
  pr.legend.text <- stringr::str_interp("Light GBM (${prauc.ci})")
  
  # build PR curve
  roc.obj <- pROC::roc(df[, target], df[, proba], plot=FALSE, legacy.axes=TRUE, percent=FALSE)
  prcoords <- coords(roc.obj, "all", ret = c("threshold", "recall", "precision"), transpose = FALSE)
  prcoords[nrow(prcoords), 3] <- 1.0000
  
  
  # bootstrap recall/precision coordinates for 95% confidence bands
  pr.cis <- ci.coords(roc.obj, prcoords$threshold, ret=c("recall", "precision"))
  pr.cis <- data.frame(pr.cis[2]) # convert precision coords to data frame
  pr.cis.df <- data.frame(x = prcoords$recall,
                       lower = pr.cis[, 1],
                       upper = pr.cis[, 3])
  
  # compute baseline (proportion of positive samples)
  npos.test = length(which(df[, target] == 1))
  nneg.test = length(which(df[, target] == 0))
  prop.pos = npos.test / (npos.test + nneg.test)
  
  ggplot(prcoords, aes(recall, precision, linetype=pr.legend.text)) + 
    geom_path(aes(recall, precision), colour=salmon, size=1) +
    geom_ribbon(aes(x=pr.cis.df$x, ymin=pr.cis.df$lower, ymax=pr.cis.df$upper), 
                alpha=0.3,
                fill=lightblue) +
    geom_hline(yintercept=prop.pos, linetype='dashed', color='gray') +
    theme(aspect.ratio = 1) +
    coord_cartesian(ylim = c(0.0, 1.0)) +
    labs(title="Precision-recall curve", x="Recall", y="Precision") +
    theme(
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(),
          panel.background = element_blank(), 
          panel.border = element_rect(colour="black", fill=NA, size=0.5),
          axis.title = element_text(size=16),
          axis.title.x = element_text(margin=margin(t=10)),
          axis.title.y = element_text(margin=margin(r=10)),
          plot.title = element_text(hjust=0.5, size=16, margin=margin(b=20)),
          legend.position = c(0.58, 0.12),
          legend.text = element_text(size=13)
          ) +
    scale_linetype_manual(name='', values=c("solid"))
}
pr.curve <- plot_pr_ci(external, "target", "proba")
pr.curve
```



```{r}
pr.curve <- plot_pr_ci(external, "target", "proba")
roc.curve <- plot_roc_ci(external, "target", "proba")
```


# Confusion matrix
```{r}
# Build confusion matrix using the cvms package
test.copy <- data.frame(external)

# convert predicted probabilities to labels using the chosen threshold
#threshold <- 0.5
threshold <- 0.5729885

test.copy$labels <- ifelse(test.copy$proba >= threshold, 1, 0)
test.copy$target <- ifelse(test.copy$target == "1", "Symptomatic", "Asymptomatic")
test.copy$labels <- ifelse(test.copy$labels == "1", "Symptomatic", "Asymptomatic")

eval <- cvms::evaluate(test.copy, target_col = "target", prediction_cols = "labels",
                       type="binomial")

cm.plot <- cvms::plot_confusion_matrix(eval, 
                                       target_col = "Target",
                                       prediction_col = "Prediction",
                                       add_row_percentages = FALSE,
                                       add_col_percentages = FALSE,
                                       palette="Blues",
                                       font_counts = cvms::font(size=6, 
                                                                color="#342d2d", 
                                                                vjust=0),
                                       font_normalized = cvms::font(size=7, 
                                                                    color="#342d2d", 
                                                                    vjust=-0.5)) +
  ggplot2::ggtitle("Predicted and actual events") +
  ggplot2::theme(axis.title = ggplot2::element_text(size=16),
                 axis.title.y = ggplot2::element_text(margin = ggplot2::margin(r=25)),
                 axis.text.x = ggplot2::element_text(size=13),
                 axis.text.y = ggplot2::element_text(size=13),
                 axis.ticks.length = ggplot2::unit(-1, "cm"),
                 plot.title = ggplot2::element_text(size=17, 
                                                    hjust=0.5, 
                                                    vjust=4.0,
                                                    family = "sans")
  ) +
  ggplot2::labs(x = "Actual", y = "Predicted")

cm.plot
```
```{r}
g <- arrangeGrob(grobs=lapply(list(roc.curve, pr.curve, cm.plot), "+", theme(plot.margin=margin(20, 20, 20, 20))), 
                 nrow=1,
                 top = textGrob("Assessment of model performance", gp=gpar(fontsize=20,font=1)))
ggsave(file="../output/plots/roc_and_pr_curves.pdf", g, width = 18, height = 6)
```

```{r}
g2 <- ggarrange(plotlist=lapply(list(roc.curve, pr.curve, cm.plot), "+", theme(plot.margin=margin(20, 20, 20, 20))),
          nrow=1,
          ncol=3,
          labels=c("A", "B", "C"),
          font.label = list(size=16))
g2 <- annotate_figure(g2, top = textGrob("Assessment of model performance", gp=gpar(fontsize=20, font=1)))
ggsave(file="../output/plots/roc_and_pr_curves.pdf", g2, width = 18, height = 6)
```


# Try plotting two curves in the same plot

```{r}
roc.obj1 <- pROC::roc(external$target, external$proba, plot=FALSE, legacy.axes=TRUE, percent=FALSE)
roc.obj2 <- pROC::roc(external$target, external$proba.demog, plot=FALSE, legacy.axes=TRUE, percent=FALSE)

prcoords1 <- coords(roc.obj1, "all", ret = c("threshold", "recall", "precision"), transpose = FALSE)
prcoords1[nrow(prcoords1), 3] <- 1.0000

prcoords2 <- coords(roc.obj2, "all", ret = c("threshold", "recall", "precision"), transpose = FALSE)
prcoords2[nrow(prcoords2), 3] <- 1.0000

prcoords.final <- merge(prcoords1, prcoords2, by="threshold")

# sto usando questi
pr.cis1 <- ci.coords(roc.obj1, prcoords1$threshold, ret=c("recall", "precision"))
pr.cis1 <- data.frame(pr.cis1[2]) # convert precision coords to data frame
pr.cis1.df <- data.frame(x = prcoords1$recall,
                     lower = pr.cis1[, 1],
                     upper = pr.cis1[, 3])

pr.cis2 <- ci.coords(roc.obj2, prcoords2$threshold, ret=c("recall", "precision"))
pr.cis2 <- data.frame(pr.cis2[2]) # convert precision coords to data frame
pr.cis2.df <- data.frame(x = prcoords2$recall,
                     lower = pr.cis2[, 1],
                     upper = pr.cis2[, 3])
```


```{r}
ggplot(prcoords1, aes(recall, precision)) + 
  geom_path(aes(recall, precision), colour=lightblue, size=1) +
  geom_ribbon(aes(x=pr.cis1.df$x, ymin=pr.cis1.df$lower, ymax=pr.cis1.df$upper), 
              alpha=0.3,
              fill=lightblue) +
  geom_path(data=prcoords2, colour=salmon, size=1) +
  geom_ribbon(data=pr.cis2.df)
  # geom_ribbon(aes(x=pr.cis2.df$x, ymin=pr.cis2.df$lower, ymax=pr.cis2.df$upper), 
  #             alpha=0.3,
  #             fill=lightblue)
``